# -*- coding: utf-8 -*-
"""
Created on Fri Mar 16 17:18:53 2018

@author: ningningzhang
"""

#1.1广义线性模型
#   w=(w1,w2,...wp) as coef_ and w0 as intercept
#1.1.1 普通的最小二乘法（线性回归）
from sklearn import linear_model
reg = linear_model.LinearRegression()
print(reg.fit([[0, 0], [1, 1], [2, 2]],[0, 1, 2]))
print(reg.coef_)
#然而，普通最小二乘估计依赖模型的独立性，当X近似奇异，将产生较大的误差
#下面是一个线性回归的例子
import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error,r2_score
diabetes = datasets.load_diabetes()
diabetes_X = diabetes.data[:, np.newaxis, 2]
diabetes_X_train = diabetes_X[: -20]
diabetes_X_test = diabetes_X[-20:]
diabetes_y_train = diabetes.target[:-20]
diabetes_y_test = diabetes.target[-20:]
regr = linear_model.LinearRegression()
regr.fit(diabetes_X_train,diabetes_y_train)
diabetes_y_pred = regr.predict(diabetes_X_test)
print('Coefficients: \n', regr.coef_)
print("Mean squared error: %.2f"
      % mean_squared_error(diabetes_y_test, diabetes_y_pred))
print('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))
plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')
plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)
plt.xticks(())
plt.yticks(())

plt.show()
###################################################################
#1.1.2 岭回归
#原来的平方损失函数加上alpha*L-2，alpha越大，对共线性的模型越robust
from sklearn import linear_model
reg = linear_model.Ridge(alpha = .5)
reg.fit([[0, 0],[0, 0],[1, 1],[0, .1, 1]])
reg.coef_
reg.intercept_
#1.1.2.2使用设置正则化参数
from sklearn import linear_model
reg = linear_model.RidgeCV(alpha = [.1, 1, 10])
reg.fit([[0, 0],[0, 0],[1, 1],[0, .1, 1]])
reg.alpha_
#################################################################
#1.1.3Lasso回归
#原来的平方损失函数加上alpha*L-1
#适用于稀疏信号，可用于特征选择
from sklearn import linear_model
reg = linear_model.Lasso(alpha = .1)
reg.predict([[1,1]])
#参数选择使用交叉验证/AIC/BIC
#与SVM正则化参数的比较
#alpha=1/C or alpha = 1/(n_samples*C)
#################################################################
#1.1.4 多任务Lasso回归



#1.1.11 Logistic回归
#他是一个分类算法，基于最大熵原理




##########################################################################
#1.1.12 随机梯度下降（SGD）
from sklearn.linear_model import SGDClassifier
X = [[0,0],[1,1]]
y = [0,1]
clf = SGDClassifier(loss="hinge",penalty="l2")
clf.fit(X,y)
clf.predict([2,2])
clf.coef_
clf.intercept_
clf.decision_function([[2,2]])
####################################################################
#1.1.13感知机
#1.不要求学习速度2.没有惩罚项3.只根据错误更新模型



######################################################################################
#1.1.16 多项式回归（扩展线性回归的基函数）
from sklearn.preprocessing import PolynomialFeatures
import numpy as np
X = np.arange(6).reshape(3,2)
poly =  PolynomialFeatures(degree=2)
poly.fit_transform(X)
